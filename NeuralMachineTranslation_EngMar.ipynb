{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralMachineTranslation_EngMar_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUuoBKSEIz1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF6F-prv0TJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAgW3Lm3I6Ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = pd.read_table('mar.txt', names=['eng', 'mar', 'fff'])\n",
        "lines = lines.iloc[:,0:2]\n",
        "lines.eng = lines.eng.apply(lambda x : x.lower())\n",
        "\n",
        "\n",
        "# Remove quotes\n",
        "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x))\n",
        "lines.mar=lines.mar.apply(lambda x: re.sub(\"'\", '', x))\n",
        "exclude = set(string.punctuation) # Set of all special characters\n",
        "\n",
        "# Remove all the special characters\n",
        "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "lines.mar=lines.mar.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "\n",
        "# Remove all numbers from text\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
        "lines.mar = lines.mar.apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
        "\n",
        "# Remove extra spaces\n",
        "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
        "lines.mar=lines.mar.apply(lambda x: x.strip())\n",
        "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "lines.mar=lines.mar.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "\n",
        "# Add start and end tokens to target sequences\n",
        "lines.mar = lines.mar.apply(lambda x : 'START '+ x + ' END')\n",
        "#lines.eng = lines.eng.apply(lambda x : 'START '+ x + ' END')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eufmo_h0JHur",
        "colab_type": "code",
        "outputId": "f77cdf1f-3f4b-42a2-f24b-5e01cd5f13ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(lines.eng.shape)\n",
        "print(lines.mar.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(36503,)\n",
            "(36503,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSQPwfPyJNBo",
        "colab_type": "code",
        "outputId": "e6017ee1-8fa7-41f3-d052-f79ae4b7cb63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RRiULZKJP5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_eng = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer_eng.fit_on_texts(lines.eng)\n",
        "word_index_eng = tokenizer_eng.word_index\n",
        "sequences_eng = tokenizer_eng.texts_to_sequences(lines.eng)\n",
        "padded_eng = pad_sequences(sequences_eng, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVaVPMzoJVIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_mar = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer_mar.fit_on_texts(lines.mar)\n",
        "word_index_mar = tokenizer_mar.word_index\n",
        "sequences_mar = tokenizer_mar.texts_to_sequences(lines.mar)\n",
        "padded_mar = pad_sequences(sequences_mar, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bus1qXLfW750",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index_mar['0']=0\n",
        "word_index_eng['0']=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEpcI_r60dzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rev_word_index_eng = dict((i, word) for word, i in word_index_eng.items())\n",
        "rev_word_index_mar = dict((i, word) for word, i in word_index_mar.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAcrSxG9JXxh",
        "colab_type": "code",
        "outputId": "28359f85-6b90-4ce0-bc9d-f6eb46ac82d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(padded_eng.shape)\n",
        "print(padded_mar.shape)\n",
        "print(len(word_index_eng))\n",
        "print(len(word_index_mar))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(36503, 34)\n",
            "(36503, 37)\n",
            "5490\n",
            "13063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArFF_2d3Jbqw",
        "colab_type": "code",
        "outputId": "db52117c-bed0-4924-bc2b-b219cb7ad9f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(type(padded_eng))\n",
        "print(type(padded_mar))\n",
        "max_length_eng = padded_eng.shape[1]\n",
        "max_length_mar = padded_mar.shape[1]\n",
        "print(max_length_eng)\n",
        "print(max_length_mar)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "34\n",
            "37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy3K0QvsJehJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUBKfc75Jibq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X, y = padded_eng, padded_mar\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjP_n1oTdvUC",
        "colab_type": "code",
        "outputId": "a1e976ef-1784-42fe-f8bd-e67f562aa063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32852, 34)\n",
            "(3651, 34)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adMFdXbILQlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, hidden_units, n_layers, dropout):\n",
        "    super().__init__()\n",
        "        \n",
        "    self.hidden_units = hidden_units\n",
        "    self.n_layers = n_layers\n",
        "       \n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.lstm = nn.LSTM(emb_dim, hidden_units, n_layers, dropout = dropout, batch_first = True)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "  def forward(self, src):   \n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "    outputs, (hidden, cell) = self.lstm(embedded)\n",
        "    return hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcBY_Z6mLeEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, hidden_units, n_layers, dropout):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_units = hidden_units\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.lstm = nn.LSTM(emb_dim, hidden_units, n_layers, dropout = dropout, batch_first = True)\n",
        "    self.fc_out = nn.Linear(hidden_units, vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self, input, hidden, cell):\n",
        "    #input = [batch size]\n",
        "    input = input.unsqueeze(1)\n",
        "    #input = [batch size, 1]\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    \n",
        "    #embedded = [batch size, 1, emb dim]\n",
        "            \n",
        "    output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "    #print('output.shape ', output.shape)\n",
        "    \n",
        "    prediction = self.fc_out(output)\n",
        "    #print('prediction.shape ', prediction.shape)\n",
        "    \n",
        "    #prediction = [batch size, 1, output dim]\n",
        "    \n",
        "    return prediction, hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clLqZto8LjrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "        \n",
        "  def forward(self, src, trg, teacher_forcing_ratio, eval):\n",
        "    \n",
        "    batch_size = trg.shape[0]\n",
        "    trg_len = trg.shape[1]\n",
        "    trg_vocab_size = self.decoder.vocab_size\n",
        "    \n",
        "\n",
        "    hidden, cell = self.encoder(src)\n",
        "    input = trg[:,0]\n",
        "\n",
        "    if eval == 1:\n",
        "      for t in range(10):\n",
        "        # input.shape  torch.Size([1]) i.e torch.Size([batch_size])\n",
        "        prediction, hidden, cell = self.decoder(input, hidden, cell)\n",
        "        # output is nothing but prediction ---->  prediction.shape  torch.Size([1, 1, 13063])\n",
        "        \n",
        "        if t==0:\n",
        "          p = prediction\n",
        "        elif t==1:\n",
        "          outputs = torch.cat((p, prediction), 1)\n",
        "        elif t>1:\n",
        "          outputs = torch.cat((outputs, prediction), 1)\n",
        "        \n",
        "        teacher_force = random.random() < teacher_forcing_ratio\n",
        "        top1 = prediction.argmax(2) \n",
        "        top1 = top1.squeeze(1)\n",
        "        input = top1\n",
        "\n",
        "    elif eval == 0:   \n",
        "      for t in range(0, trg_len-1):\n",
        "        # input.shape  torch.Size([128]) i.e torch.Size([batch_size])\n",
        "        output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "        # output is nothing but prediction ---->  prediction.shape  torch.Size([128, 1, 13063])\n",
        "        \n",
        "        if t==0:\n",
        "          p = output\n",
        "        elif t==1:\n",
        "          outputs = torch.cat((p, output), 1)\n",
        "        elif t>1:\n",
        "          outputs = torch.cat((outputs, output), 1)\n",
        "        \n",
        "        teacher_force = random.random() < teacher_forcing_ratio\n",
        "        top1 = output.argmax(2)\n",
        "        top1 = top1.squeeze(1)\n",
        "        input = trg[:, t+1] if teacher_force else top1\n",
        "    \n",
        "    outputs.to(self.device)\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aHF-8mOMPbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size_eng = len(word_index_eng)\n",
        "vocab_size_mar = len(word_index_mar)\n",
        "emb_dim = 256\n",
        "hidden_units = 512\n",
        "n_layers = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "batch_size = 128\n",
        "\n",
        "encoder = Encoder(vocab_size_eng, emb_dim, hidden_units, n_layers, ENC_DROPOUT)\n",
        "decoder = Decoder(vocab_size_mar, emb_dim, hidden_units, n_layers, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Avpn7m-0RSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzB2Qv_NqwDk",
        "colab_type": "code",
        "outputId": "06125a8a-dba7-4ee0-f523-87619c0b0b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TRAINING \n",
        "src = X_train\n",
        "trg = y_train\n",
        "src = torch.tensor(src)\n",
        "trg = torch.tensor(trg)\n",
        "src = src.long()\n",
        "trg = trg.long()\n",
        "src = src.to(device)\n",
        "trg = trg.to(device)\n",
        "\n",
        "end = int(X_train.shape[0]/batch_size)\n",
        "print(end)\n",
        "\n",
        "for epoch in range(100):\n",
        "  epoch_loss = 0\n",
        "  for i in range(  end ):   \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    a = src[ (0 + i*batch_size) : (  (i+1) * batch_size ) ,  : ]\n",
        "    b = trg[ (0 + i*batch_size) : (  (i+1) * batch_size ) ,  : ]\n",
        "\n",
        "    output = model(a, b, 1, 0)\n",
        "    output = output.view(  batch_size*(max_length_mar - 1), len(word_index_mar)  )\n",
        "    \n",
        "    vocab_size_mar = output.shape[1]\n",
        "    \n",
        "    b = b[:, 1:]\n",
        "    b = b.reshape(  (max_length_mar-1) * batch_size,  )\n",
        "    \n",
        "    \n",
        "    loss = F.cross_entropy(output, b)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  print(\"epoch loss : \", epoch, ' ',  epoch_loss/end)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n",
            "epoch loss :  0   1.0438387931790203\n",
            "epoch loss :  1   0.8356543455738574\n",
            "epoch loss :  2   0.7313334529753774\n",
            "epoch loss :  3   0.6362467114813626\n",
            "epoch loss :  4   0.5524572368012741\n",
            "epoch loss :  5   0.48183869605418295\n",
            "epoch loss :  6   0.42273825756274164\n",
            "epoch loss :  7   0.37268149852752686\n",
            "epoch loss :  8   0.3304470519069582\n",
            "epoch loss :  9   0.29504168493440375\n",
            "epoch loss :  10   0.2647537913871929\n",
            "epoch loss :  11   0.240221394225955\n",
            "epoch loss :  12   0.21832720295060426\n",
            "epoch loss :  13   0.19955158763332292\n",
            "epoch loss :  14   0.18343240965623409\n",
            "epoch loss :  15   0.1695384561899118\n",
            "epoch loss :  16   0.15666136422078125\n",
            "epoch loss :  17   0.1448961968999356\n",
            "epoch loss :  18   0.13558041770011187\n",
            "epoch loss :  19   0.12688005427480675\n",
            "epoch loss :  20   0.11913271885714494\n",
            "epoch loss :  21   0.11086317172157578\n",
            "epoch loss :  22   0.10492519781109877\n",
            "epoch loss :  23   0.09881433265400119\n",
            "epoch loss :  24   0.09276260642218404\n",
            "epoch loss :  25   0.08798053406644613\n",
            "epoch loss :  26   0.08387610345380381\n",
            "epoch loss :  27   0.07980462726845872\n",
            "epoch loss :  28   0.0758418740297202\n",
            "epoch loss :  29   0.07222066019312479\n",
            "epoch loss :  30   0.06966677677701227\n",
            "epoch loss :  31   0.06714192245271988\n",
            "epoch loss :  32   0.06475082361430395\n",
            "epoch loss :  33   0.06269950729620177\n",
            "epoch loss :  34   0.06040285048948135\n",
            "epoch loss :  35   0.0579769824835239\n",
            "epoch loss :  36   0.05636628082720563\n",
            "epoch loss :  37   0.05471407303411979\n",
            "epoch loss :  38   0.05366443982347846\n",
            "epoch loss :  39   0.05173085931164678\n",
            "epoch loss :  40   0.050003414376988076\n",
            "epoch loss :  41   0.04884050322289113\n",
            "epoch loss :  42   0.04775569195044227\n",
            "epoch loss :  43   0.04657225636765361\n",
            "epoch loss :  44   0.045906028142781\n",
            "epoch loss :  45   0.04463588775251992\n",
            "epoch loss :  46   0.04445654875598848\n",
            "epoch loss :  47   0.043792990036308765\n",
            "epoch loss :  48   0.04310095153050497\n",
            "epoch loss :  49   0.0424318481964292\n",
            "epoch loss :  50   0.04236501287959982\n",
            "epoch loss :  51   0.0422756263287738\n",
            "epoch loss :  52   0.04219265225401614\n",
            "epoch loss :  53   0.04188005707692355\n",
            "epoch loss :  54   0.04135225334903225\n",
            "epoch loss :  55   0.04040015123609919\n",
            "epoch loss :  56   0.03959754698735196\n",
            "epoch loss :  57   0.03941597864468349\n",
            "epoch loss :  58   0.03911416672053747\n",
            "epoch loss :  59   0.03931786840985296\n",
            "epoch loss :  60   0.04019888438779162\n",
            "epoch loss :  61   0.04079185579030309\n",
            "epoch loss :  62   0.039097486122045666\n",
            "epoch loss :  63   0.038108648601337336\n",
            "epoch loss :  64   0.03877017755439738\n",
            "epoch loss :  65   0.0421548759622965\n",
            "epoch loss :  66   0.046782863631960936\n",
            "epoch loss :  67   0.046449253015452996\n",
            "epoch loss :  68   0.044895459912368096\n",
            "epoch loss :  69   0.042501728865318\n",
            "epoch loss :  70   0.041162103007081896\n",
            "epoch loss :  71   0.039969991477846634\n",
            "epoch loss :  72   0.03969355820299825\n",
            "epoch loss :  73   0.03936201363103464\n",
            "epoch loss :  74   0.038160090160090476\n",
            "epoch loss :  75   0.03776328929234296\n",
            "epoch loss :  76   0.037171587529883254\n",
            "epoch loss :  77   0.03823975742125185\n",
            "epoch loss :  78   0.03775841936294455\n",
            "epoch loss :  79   0.03687349489337066\n",
            "epoch loss :  80   0.035869715866283514\n",
            "epoch loss :  81   0.03760453966242494\n",
            "epoch loss :  82   0.03909908919740701\n",
            "epoch loss :  83   0.03827635772177018\n",
            "epoch loss :  84   0.03720803062606137\n",
            "epoch loss :  85   0.03622887066012481\n",
            "epoch loss :  86   0.03537235849944409\n",
            "epoch loss :  87   0.03372688543458935\n",
            "epoch loss :  88   0.03324466686899541\n",
            "epoch loss :  89   0.032481655915034935\n",
            "epoch loss :  90   0.031687293128925376\n",
            "epoch loss :  91   0.031099827428988647\n",
            "epoch loss :  92   0.030512327488395385\n",
            "epoch loss :  93   0.030186703712388407\n",
            "epoch loss :  94   0.03263503393100109\n",
            "epoch loss :  95   0.031836019072216004\n",
            "epoch loss :  96   0.03141795231204014\n",
            "epoch loss :  97   0.03072034400975099\n",
            "epoch loss :  98   0.030144792945066\n",
            "epoch loss :  99   0.03002833961363649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb8s4o5M17lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'nmt_weight.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spI9Qk_dfair",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "model.load_state_dict(torch.load('nmt_weight.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oamPzJUvQwKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TESTING\n",
        "src = X_train\n",
        "trg = y_train\n",
        "src = torch.tensor(src)\n",
        "trg = torch.tensor(trg)\n",
        "src = src.long()\n",
        "trg = trg.long()\n",
        "src = src.to(device)\n",
        "trg = trg.to(device)\n",
        "\n",
        "end = int(X_test.shape[0]/batch_size)\n",
        "print(end)\n",
        "\n",
        "for epoch in range(10):\n",
        "  epoch_loss = 0\n",
        "  for i in range(  end ):   \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    a = src[ (0 + i*batch_size) : (  (i+1) * batch_size ) ,  : ]\n",
        "    b = trg[ (0 + i*batch_size) : (  (i+1) * batch_size ) ,  : ]\n",
        "\n",
        "    output = model(a, b, 0, 0)\n",
        "    output = output.view(  batch_size*(max_length_mar - 1) , len(word_index_mar) )\n",
        "    \n",
        "    vocab_size_mar = output.shape[1]\n",
        "    \n",
        "    b = b[:, 1:]\n",
        "    b = b.reshape( (max_length_mar - 1)*batch_size,)\n",
        "    \n",
        "    \n",
        "    loss = F.cross_entropy(output, b)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  print(\"epoch loss : \", epoch, ' ',  epoch_loss/20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c5Fz9GRBxTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translation(eng_sent):\n",
        "  eng_sent = pd.Series(eng_sent)\n",
        "  eng_sent = eng_sent.apply(lambda x : x.lower())\n",
        "  sequences_eng_sent = tokenizer_eng.texts_to_sequences(eng_sent)\n",
        "  padded_eng_sent = pad_sequences(sequences_eng_sent, padding='post')\n",
        "  padded_eng_sent = torch.tensor(padded_eng_sent)\n",
        "  padded_eng_sent = padded_eng_sent.long()\n",
        "  padded_eng_sent = padded_eng_sent.to(device)\n",
        "  c = torch.tensor([[ word_index_mar['start'] ]]).long()\n",
        "  c = c.to(device)\n",
        "  output = model(padded_eng_sent, c, 0, 1)\n",
        "  q = output.argmax(2)\n",
        "  q = q.squeeze(0)\n",
        "  translated = ''\n",
        "  for i in q:\n",
        "    translated = translated + ' ' +  rev_word_index_mar[int(i)]  \n",
        "  return translated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35segr6RnDKS",
        "colab_type": "code",
        "outputId": "c8440ff5-b68b-41c3-de80-7edb8cdce327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "eng_sent = 'i will give you this book' # Your Engish Sentence\n",
        "sentence = translation(eng_sent)\n",
        "print(sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " मी तुला एक घर देईन end 0 0 0 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7M0OjE7qrhs",
        "colab_type": "code",
        "outputId": "83e6527b-153f-4c40-bae7-2202a112f627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "eng_sent = 'tom was eating a sandwich' # Your Engish Sentence\n",
        "sentence = translation(eng_sent)\n",
        "print(sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " टॉम एक कप होता end 0 0 0 0 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vu8Zqxs0cyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}